{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ded2024-0efb-417e-b5d9-d11b3ad0142c",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vischia/ecai2025/blob/master/hello_uncertainty_quantification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55064c63",
   "metadata": {},
   "source": [
    "\n",
    "# Think Your Deep Learning Model Works? Think Again!!! — Hands-on (PyTorch)\n",
    "\n",
    "## P. Vischia (pietro.vischia@cern.ch)\n",
    "\n",
    "This tutorial assumes you have read [my Part 2 Lecture](https://www.hep.uniovi.es/vischia/persistent/2025-10-26_TutorialECAI2025.html), and possibly also [the lectures from Part 1 by Miriam](https://sites.google.com/view/dl-common-pitfalls/materials?authuser=0)\n",
    "\n",
    "This tutorial demonstrates how:\n",
    "- Building a small PyTorch model and **good vs bad** data splits\n",
    "- **Deep ensembles** and a sketch of **temperature scaling**\n",
    "- **Split Conformal Prediction** (CQR-style) for regression\n",
    "- **Weighted Conformal** under covariate shift via a learned density ratio\n",
    "- **OOD scoring** with **Energy** and **ODIN-style** perturbation\n",
    "- **Regularization/model size sweep** to discuss double-descent-ish behavior\n",
    "- **Demonstration of leakage in conformal prediction**\n",
    "- A tiny example of **injecting inductive bias** (monotonicity penalty)\n",
    "- A larger example of **injecting inductive bias**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e8c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_default_dtype(torch.float32) # if you are on Mac M1--4\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Small utility: simple training loop for MSE\n",
    "def train_epoch_mse(model, opt, dl):\n",
    "    model.train()\n",
    "    for xb, yb in dl:\n",
    "        opt.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = F.mse_loss(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "def plot_hist(data, title, xlabel):\n",
    "    plt.figure()\n",
    "    plt.hist(data, bins=40)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_hists(data, title, xlabel):\n",
    "    plt.figure()\n",
    "    for d, l in zip(data, title):\n",
    "        plt.hist(d, bins=40, label=l)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2697c60-462c-4e16-98a4-78adb32783a2",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "Let's start by creating a synthetic data set, made of:\n",
    "\n",
    "- A training data set: $X \\sim Gaus(0,1)$\n",
    "- A test data set with a covariate shift: $X \\sim Gaus(1.5,1)$\n",
    "- A target label: $y = sin(x + \\epsilon_1) + \\epsilon_2$, with $\\epsilon_1 < \\epsilon_2$\n",
    "- Group labels to illustrate leakage (e.g., sites/users): $randint(0,10)$\n",
    "\n",
    "### Exercise\n",
    "Play with the generated data set by changing its parameters and see how this affects everything that follows. For instance, try with $\\epsilon_1 > \\epsilon_2$, try with a larger or smaller covariate shift, try with a larger or smaller number of training events, and so on and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac1650",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6000 # Training events. Test events are 1/5 of those (usual 80/20 splitting)\n",
    "x_train = torch.randn(n, 1)\n",
    "y_train = (x_train + 0.3*torch.randn(n,1)).sin() + 0.1*torch.randn(n,1)\n",
    "\n",
    "x_test  = torch.randn(n//5, 1) + 1.5\n",
    "y_test  = (x_test + 0.3*torch.randn(n//5,1)).sin() + 0.1*torch.randn(n//5,1)\n",
    "\n",
    "groups = torch.randint(0, 10, (n,))\n",
    "groups_test = torch.randint(0, 10, (n//5,))\n",
    "\n",
    "def get_loaders(x, y, batch=128, shuffle=True):\n",
    "    ds = TensorDataset(x, y)\n",
    "    return DataLoader(ds, batch_size=batch, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cfc289-9941-4e0f-a26f-54d3ea00184d",
   "metadata": {},
   "source": [
    "Now we want to try two ways of splitting the dataset.\n",
    "\n",
    "Data leakage can happen when group-related features appear in both training and test sets. For instance, multiple data on a same patient (for instance multiple visits in time) must not be split among training and testing data set.\n",
    "\n",
    "*Data Leakage is a huge problem for time series data*\n",
    "\n",
    "You must take care that data related to the same group (e.g. a patient) appears *either* in the training set *or* the test set, but *not both*.\n",
    "\n",
    "This kind of data leakage may lead to estimating the performance of the model too optimistically.\n",
    "\n",
    "How to cope with this?\n",
    "- The Bad Way: ignore the groups, and do the usual random shuffle. **this can cause leakage**\n",
    "- The Good Way: hold out entire groups for either training or test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c824a7a-afee-4886-bc45-555404798d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD split: ignore groups, classical random shuffle-split (can leak)\n",
    "bad_loader = get_loaders(x_train, y_train, shuffle=True)\n",
    "\n",
    "# GOOD split exemplar: hold out entire groups -> emulate sklearn.GroupShuffleSplit\n",
    "mask_holdout_groups = groups < 2  # groups {0,1} go to holdout\n",
    "good_train_mask = ~mask_holdout_groups\n",
    "dl_good = get_loaders(x_train[good_train_mask], y_train[good_train_mask], shuffle=True)\n",
    "\n",
    "# Quick visualization of distributions\n",
    "plt.figure()\n",
    "plt.hist(x_train.numpy(), bins=50, alpha=0.7, label=\"train X\")\n",
    "plt.hist(x_test.numpy(), bins=50, alpha=0.7, label=\"test X (shifted)\")\n",
    "plt.legend()\n",
    "plt.title(\"Covariate Shift: Train vs Test X\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20068334-33ad-4867-b826-de5bda5292c0",
   "metadata": {},
   "source": [
    "## Deep Ensembles\n",
    "\n",
    "As we saw in the slides, we want to build an ensembre of neural network.\n",
    "\n",
    "Logically, we then start by defining a small Multilayer Perceptron class for regressing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478683f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d=1, h=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, h), nn.ReLU(),\n",
    "            nn.Linear(h, h), nn.ReLU(),\n",
    "            nn.Linear(h, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def evaluate_mse(model, x, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return F.mse_loss(model(x), y).item()\n",
    "\n",
    "baseline = MLP()\n",
    "opt = torch.optim.AdamW(baseline.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "for _ in range(30):\n",
    "    train_epoch_mse(baseline, opt, bad_loader)    # intentionally \"bad\" split to show optimism\n",
    "\n",
    "mse_bad = evaluate_mse(baseline, x_test, y_test)\n",
    "print(\"Baseline test MSE (bad split trained):\", mse_bad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf9013-a49b-4c5d-a5ca-6485543a7db7",
   "metadata": {},
   "source": [
    "Now we are ready to do a Deep Ensemble (as in the slides, let's ensemble $5$ classifiers\n",
    "\n",
    "### Exercise\n",
    "Try using more classifiers, or less classifiers, and see how the reported uncertainty varies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f9d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ensemble_member(seed, loader, epochs=30, h=64):\n",
    "    torch.manual_seed(seed)\n",
    "    m = MLP(h=h)\n",
    "    opt = torch.optim.AdamW(m.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    for _ in range(epochs):\n",
    "        train_epoch_mse(m, opt, loader)\n",
    "    return m\n",
    "\n",
    "E = [fit_ensemble_member(s, bad_loader) for s in range(5)]\n",
    "\n",
    "@torch.no_grad()\n",
    "def ens_predict(models, x):\n",
    "    preds = torch.stack([m(x).squeeze(-1) for m in models], dim=0)  # [M, N]\n",
    "    mean = preds.mean(0)\n",
    "    var_epistemic = preds.var(0, unbiased=False)\n",
    "    return mean, var_epistemic, preds\n",
    "\n",
    "mu_bad, var_ep_bad, preds_bad = ens_predict(E, x_test)\n",
    "\n",
    "# Plot predictive mean + a band from ensemble variance\n",
    "plt.figure()\n",
    "order = torch.argsort(x_test.squeeze(-1))\n",
    "plt.plot(x_test[order].numpy(), y_test[order].numpy(), \".\", alpha=0.3, label=\"y_test\")\n",
    "plt.plot(x_test[order].numpy(), mu_bad[order].unsqueeze(-1).numpy(), \"-\", label=\"Ensemble mean\")\n",
    "std = var_ep_bad.sqrt().unsqueeze(-1)\n",
    "plt.fill_between(x_test[order].squeeze(-1).numpy(),\n",
    "                 (mu_bad[order]-2*std[order].squeeze(-1)).numpy(),\n",
    "                 (mu_bad[order]+2*std[order].squeeze(-1)).numpy(),\n",
    "                 alpha=0.3, label=\"~95% ensemble band\")\n",
    "plt.title(\"Deep Ensemble: mean and (approx) uncertainty\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7719d0-a465-41e0-b093-2adc8bbe860c",
   "metadata": {},
   "source": [
    "### Temperature scaling\n",
    "\n",
    "Reminder: temperature scaling consists in building a logistic regression on the logits and training it to output calibrated probabilities. This results in changing the *confidence* but *not the accuracy*, which remains constant.\n",
    "\n",
    "### Exercise\n",
    "- Use the sketch below of the TemperatureHead to retrain the DeepEnsemble.\n",
    "- Then draw the reliability diagrams for the DeepEnsemble without temperature scaling and for the one with temperature scaling, and compare them\n",
    "- Change the temperature and see how that affects the confidence (and verifies it does not change the accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf2f28-115a-4124-9a12-d373f1fa8b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature head sketch\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.T = nn.Parameter(torch.ones(()))\n",
    "\n",
    "def temp_scale(logits, T):\n",
    "    return logits / T.clamp_min(1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f700a61-06ba-4b24-9d44-f0799341bc75",
   "metadata": {},
   "source": [
    "## Conformal prediction\n",
    "\n",
    "We will now use Conformalized Quantile Qegression (CQR) to train a model to predict the lower and upper conditional quantiles and then calibrate the slack (the difference w.r.t. the target coverage)\n",
    "\n",
    "We'll the print the nominal (target) coverage and the empirical coverage (to see if we over/under cover).\n",
    "\n",
    "### Exercise\n",
    "Play with the nominal coverage.\n",
    "- Plot of empirical vs nominal coverage as a function of the nominal coverage\n",
    "- Run many times to build the full distribution of the empirical coverage, and make an histogram of it, with a vertical line indicating the nominal coverage. See if you get a distribution similar to that in Slide 36."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26afc338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a proper split for conformal: train/calibration (no peeking)\n",
    "idx = torch.randperm(len(x_train))\n",
    "n_cal = len(x_train)//5\n",
    "idx_cal, idx_tr = idx[:n_cal], idx[n_cal:]\n",
    "x_tr, y_tr = x_train[idx_tr], y_train[idx_tr]\n",
    "x_cal, y_cal = x_train[idx_cal], y_train[idx_cal]\n",
    "\n",
    "def get_loader_xy(x, y, batch=128, shuffle=True):\n",
    "    return DataLoader(TensorDataset(x, y), batch_size=batch, shuffle=shuffle)\n",
    "\n",
    "dl_tr = get_loader_xy(x_tr, y_tr)\n",
    "\n",
    "# Quantile model\n",
    "class QuantileMLP(nn.Module):\n",
    "    def __init__(self, d=1, h=64):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(nn.Linear(d,h), nn.ReLU(), nn.Linear(h,h), nn.ReLU())\n",
    "        self.q_lo = nn.Linear(h,1)  # tau=0.1\n",
    "        self.q_hi = nn.Linear(h,1)  # tau=0.9\n",
    "    def forward(self, x):\n",
    "        h = self.f(x)\n",
    "        return self.q_lo(h), self.q_hi(h)\n",
    "\n",
    "def pinball(yhat, y, q):\n",
    "    e = y - yhat\n",
    "    return torch.mean(torch.maximum(q*e, (q-1)*e))\n",
    "\n",
    "qnet = QuantileMLP()\n",
    "optq = torch.optim.Adam(qnet.parameters(), lr=1e-3)\n",
    "for epoch in range(40):\n",
    "    for xb, yb in dl_tr:\n",
    "        optq.zero_grad()\n",
    "        ql, qh = qnet(xb)\n",
    "        loss = pinball(ql, yb, 0.1) + pinball(qh, yb, 0.9)\n",
    "        loss.backward()\n",
    "        optq.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_q(x):\n",
    "    ql, qh = qnet(x)\n",
    "    return ql.squeeze(-1), qh.squeeze(-1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def cqr_intervals(x_new, alpha=0.1):\n",
    "    ql, qh = predict_q(x_cal)\n",
    "    s = torch.maximum(ql - y_cal.squeeze(-1), y_cal.squeeze(-1) - qh)   # CQR score\n",
    "    qhat = torch.quantile(s, 1 - alpha, interpolation='higher')\n",
    "    ql_new, qh_new = predict_q(x_new)\n",
    "    return (ql_new - qhat, qh_new + qhat)\n",
    "\n",
    "L, U = cqr_intervals(x_test, alpha=0.1)\n",
    "\n",
    "# Visualize intervals on a subset\n",
    "idx_plot = torch.argsort(x_test.squeeze(-1))[:400]\n",
    "xp, yp = x_test[idx_plot], y_test[idx_plot]\n",
    "Lp, Up = L[idx_plot], U[idx_plot]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xp.numpy(), yp.numpy(), \".\", alpha=0.5, label=\"y_test\")\n",
    "plt.plot(xp.numpy(), ((Lp+Up)/2).unsqueeze(-1).numpy(), \"-\", label=\"center\")\n",
    "for i in range(len(xp)):\n",
    "    plt.plot([xp[i].item(), xp[i].item()], [Lp[i].item(), Up[i].item()], alpha=0.3)\n",
    "plt.title(\"Split Conformal (CQR-style) Intervals (subset)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "@torch.no_grad()\n",
    "def coverage(y_true, L, U):\n",
    "    return ((y_true.squeeze(-1) >= L) & (y_true.squeeze(-1) <= U)).float().mean().item()\n",
    "\n",
    "cov = coverage(y_test, L, U)\n",
    "print(\"Approx. marginal coverage on shifted test (nominal 0.9):\", round(cov, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae108e-aefd-4e79-82c8-7dcc0fdf1a14",
   "metadata": {},
   "source": [
    "## Conformal prediction under covariate shift\n",
    "\n",
    "Reminder: we will want to upweight the conformal scores from calibration points that would be more likely under the new distribution, using the weight:\n",
    "\n",
    "$w(x)=\\frac{dP\\_{test}(x)}{dP/(x)}$\n",
    "\n",
    "and then we will use the weighted quantile.\n",
    "\n",
    "### Exercise\n",
    "- Compare the weighted coverage with the nonweighted coverage (the one that doesn't account for the covariate shift)\n",
    "- Change the definition of the weight and watch the world crumble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c929ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "x_p = x_cal                       # calibration features from P\n",
    "x_q = x_test                      # test covariates from Q (assume available at calibration)\n",
    "Xp = torch.cat([x_p, x_q], dim=0)\n",
    "y_domain = torch.cat([torch.zeros(len(x_p)), torch.ones(len(x_q))]).long()\n",
    "\n",
    "dom_net = nn.Sequential(nn.Linear(1,32), nn.ReLU(), nn.Linear(32,2))\n",
    "optD = torch.optim.Adam(dom_net.parameters(), lr=1e-3)\n",
    "\n",
    "dlD = DataLoader(TensorDataset(Xp, y_domain), batch_size=128, shuffle=True)\n",
    "for _ in range(20):\n",
    "    for xb, yb in dlD:\n",
    "        optD.zero_grad()\n",
    "        logits = dom_net(xb)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "        loss.backward()\n",
    "        optD.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def density_ratio(x):\n",
    "    logits = dom_net(x)\n",
    "    p = F.softmax(logits, dim=-1)\n",
    "    p_q = p[:,1]\n",
    "    p_p = p[:,0]\n",
    "    ratio = (p_q / p_p).clamp(1e-3, 1e3)\n",
    "    return ratio\n",
    "\n",
    "@torch.no_grad()\n",
    "def weighted_cqr_intervals(x_new, alpha=0.1):\n",
    "    ql, qh = predict_q(x_cal)\n",
    "    s = torch.maximum(ql - y_cal.squeeze(-1), y_cal.squeeze(-1) - qh)\n",
    "    w = density_ratio(x_cal)\n",
    "    w = w / w.sum()\n",
    "    s_sorted, idx = torch.sort(s)\n",
    "    w_sorted = w[idx]\n",
    "    cdf = torch.cumsum(w_sorted, dim=0)\n",
    "    pos = (cdf >= 1 - alpha).nonzero()[0]\n",
    "    qhat = s_sorted[pos]\n",
    "    ql_new, qh_new = predict_q(x_new)\n",
    "    return (ql_new - qhat, qh_new + qhat)\n",
    "\n",
    "Lw, Uw = weighted_cqr_intervals(x_test, alpha=0.1)\n",
    "\n",
    "cov_w = coverage(y_test, Lw, Uw)\n",
    "print(\"Weighted CP coverage on shifted test (nominal 0.9):\", round(cov_w, 3))\n",
    "\n",
    "# Compare interval widths\n",
    "width_std = (U - L).mean().item()\n",
    "width_w   = (Uw - Lw).mean().item()\n",
    "print(\"Avg width (Standard CP):\", round(width_std, 3))\n",
    "print(\"Avg width (Weighted CP):\", round(width_w, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41b68a-2668-4636-a14f-eb19d1aa8754",
   "metadata": {},
   "source": [
    "## Out of Distribution (OOD) Shenanigans\n",
    "\n",
    "We'll apply two OOD detection algorithms: ODIN and the Energy one.\n",
    "\n",
    "We start with a small classifier for our synthetic regression labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeccdfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(1,64), nn.ReLU(), nn.Linear(64,2))\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "clf = Classifier()\n",
    "optC = torch.optim.AdamW(clf.parameters(), lr=5e-4)\n",
    "y_cls = (y_train.squeeze(-1) > 0).long()\n",
    "dl_cls = DataLoader(TensorDataset(x_train, y_cls), batch_size=128, shuffle=True)\n",
    "\n",
    "for _ in range(25):\n",
    "    for xb, yb in dl_cls:\n",
    "        optC.zero_grad()\n",
    "        logits = clf(xb)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "        loss.backward()\n",
    "        optC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6998e4b5-ba36-4d79-b4a6-d4a68e14e311",
   "metadata": {},
   "source": [
    "Now we are ready to calculate the Energy and ODIN OOD detectors\n",
    "\n",
    "### Exercise\n",
    "Play with the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4782cfb-885f-41fd-9fb2-76403b1c0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def energy_score(x):\n",
    "    logits = clf(x)\n",
    "    return -torch.logsumexp(logits, dim=-1)\n",
    "\n",
    "E_in  = energy_score(x_train[:2000]).numpy()\n",
    "E_ood = energy_score(x_test[:2000]).numpy()\n",
    "\n",
    "plot_hists([E_in, E_ood], [\"Energy (approx. in-distribution)\",\"Energy (shifted/OOD proxy)\"], \"Energy\")\n",
    "\n",
    "# ODIN-style: temperature + small input perturbation\n",
    "def odin_score(x, eps=0.001, T=2.0):\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    logits = clf(x)/T\n",
    "    yhat = logits.argmax(dim=-1)\n",
    "    loss = F.cross_entropy(logits, yhat)\n",
    "    loss.backward()\n",
    "    x_pert = x - eps * x.grad.sign()\n",
    "    with torch.no_grad():\n",
    "        logits_p = clf(x_pert)/T\n",
    "        msp = F.softmax(logits_p, dim=-1).max(dim=-1).values\n",
    "    return 1 - msp  # larger => more likely OOD\n",
    "\n",
    "ODIN_in  = odin_score(x_train[:2000]).detach().numpy()\n",
    "ODIN_ood = odin_score(x_test[:2000]).detach().numpy()\n",
    "\n",
    "plot_hists([ODIN_in, ODIN_ood], [\"ODIN score (approx. in-distribution)\",\"ODIN score (shifted/OOD proxy)\"], \"1 - MSP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61c4000-4171-481d-a3f7-3625d3abfe70",
   "metadata": {},
   "source": [
    "## Regularization and model size sweep\n",
    "### (a vulgar attempt at reproducing the double-descent behaviour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f9e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(width, wd, epochs=60):\n",
    "    m = MLP(h=width)\n",
    "    opt = torch.optim.AdamW(m.parameters(), lr=1e-3, weight_decay=wd)\n",
    "    for _ in range(epochs):\n",
    "        train_epoch_mse(m, opt, dl_good)   # use \"good\" split now\n",
    "    with torch.no_grad():\n",
    "        mse = F.mse_loss(m(x_test), y_test).item()\n",
    "    return mse\n",
    "\n",
    "widths = [8, 16, 32, 64, 128, 256, 512]\n",
    "wds    = [0.0, 1e-5, 1e-4, 1e-3]\n",
    "results = np.zeros((len(wds), len(widths)))\n",
    "\n",
    "for i, wd in enumerate(wds):\n",
    "    for j, w in enumerate(widths):\n",
    "        results[i, j] = train_and_eval(w, wd)\n",
    "\n",
    "# Plot: one figure with lines for each weight decay over width\n",
    "plt.figure()\n",
    "for i, wd in enumerate(wds):\n",
    "    plt.plot(widths, results[i], marker=\"o\", label=f\"weight_decay={wd}\")\n",
    "plt.xlabel(\"Width (hidden units)\")\n",
    "plt.ylabel(\"Test MSE\")\n",
    "plt.title(\"Model size × regularization sweep\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5157d585-33aa-4170-891d-df17b4d9f167",
   "metadata": {},
   "source": [
    "## Leakage and validity of conformal prediction\n",
    "\n",
    "*DO NOT DO THIS AT HOME: IT IS WRONG, WE DO IT HERE ONLY TO ILLUSTRATE HOW BAD IT IS:* We'll calculate conformal prediction intervals using the test distribution in calibration, and compare to what happens when we properly compute the conformal prediction intervals strictly from the training distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD: DO NOT DO THIS EVER\n",
    "Lb, Ub = cqr_intervals(x_test, alpha=0.1)  # wrong: calibration polluted by test covariates\n",
    "cov_bad = coverage(y_test, Lb, Ub)\n",
    "\n",
    "# GOOD: DO THIS INSTEAD\n",
    "Lg, Ug = cqr_intervals(x_train[idx_cal][:len(x_test)], alpha=0.1) # Only training events are used\n",
    "cov_good = coverage(y_test, Lg, Ug)\n",
    "\n",
    "print(\"Coverage (BAD split / leakage):\", round(cov_bad, 3))\n",
    "print(\"Coverage (GOOD split):          \", round(cov_good, 3))\n",
    "\n",
    "# Plot bar comparison\n",
    "plt.figure()\n",
    "plt.bar([\"Bad (leakage)\", \"Good\"], [cov_bad, cov_good])\n",
    "plt.axhline(0.9, linestyle=\"--\")\n",
    "plt.title(\"Coverage comparison (nominal 0.9)\")\n",
    "plt.ylabel(\"Observed coverage\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d22bf-1702-4aca-ab64-eb7ff07114a9",
   "metadata": {},
   "source": [
    "## Simple inductive bias: a monotonicity penalty\n",
    "\n",
    "We'll introduce a monotonic penalty, and see how it changes the prediction from our basic MLP\n",
    "\n",
    "### Exercise\n",
    "- Play with the strength of the penalty (the `1e-2`) to stronger and weaker values\n",
    "- Change the initial dataset playing with its parameters and entity of covariate shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce94e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monotone_penalty(model, xs):\n",
    "    xs = xs.clone().detach().requires_grad_(True)\n",
    "    y = model(xs)\n",
    "    grad = torch.autograd.grad(y.sum(), xs, create_graph=True)[0]\n",
    "    return F.relu(-grad).mean()\n",
    "\n",
    "m_mono = MLP()\n",
    "opt_mono = torch.optim.AdamW(m_mono.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "for _ in range(30):\n",
    "    for xb, yb in dl_good:\n",
    "        opt_mono.zero_grad()\n",
    "        pred = m_mono(xb)\n",
    "        loss = F.mse_loss(pred, yb) + 1e-2 * monotone_penalty(m_mono, xb)\n",
    "        loss.backward()\n",
    "        opt_mono.step()\n",
    "\n",
    "mse_mono = evaluate_mse(m_mono, x_test, y_test)\n",
    "print(\"Test MSE with monotonicity penalty:\", round(mse_mono, 4))\n",
    "\n",
    "# Visualize predictions vs baseline at a grid\n",
    "grid = torch.linspace(x_test.min().item()-1, x_test.max().item()+1, 400).unsqueeze(-1)\n",
    "with torch.no_grad():\n",
    "    y_base = baseline(grid)\n",
    "    y_mono = m_mono(grid)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(grid.numpy(), y_base.numpy(), label=\"baseline\")\n",
    "plt.plot(grid.numpy(), y_mono.numpy(), label=\"monotone-penalized\")\n",
    "plt.title(\"Effect of inductive bias (monotonicity penalty)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b909a-1d4a-4b5e-ad3a-bba4a65290b1",
   "metadata": {},
   "source": [
    "## Inductive bias in a real case\n",
    "\n",
    "Implement the inductive bias we used for this paper [Phys. Rev. D 110, 096023](https://doi.org/10.1103/PhysRevD.110.096023).\n",
    "\n",
    "How? We have put the code freely online: [https://github.com/sscruz/eq_CP/](https://github.com/sscruz/eq_CP/)!!!\n",
    "\n",
    "We want to enforce equivariance with respect to a symmetry called Charge-Parity (CP).\n",
    "\n",
    "- We write down the most general equivariant function under CP:\n",
    "    $f(event) = g(event) - g(CP(event))$\n",
    "- We parameterize $g$ using a neural network, then train $f$ to minimize a loss function\n",
    "  - After training score is CP-odd (even) for CP-odd (even) processes\n",
    "- Injected information results in <b>40-300% less iterations</b> needed to achieve the same loss value!!!\n",
    "\n",
    "It's very simple!\n",
    "\n",
    "We define a `self.main_module` that is a regular sequential model (MLP), and instead of returning `self.main_module(x)` we simply return `self.main_module(x)-self.main_module(cpx)`, where `cpx` is the data point transformed according to the CP symmetry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e762fe",
   "metadata": {},
   "source": [
    "## Have fun, and have a great ECAI 2025!!!\n",
    "\n",
    "We will :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5786ae3e-875d-48b5-b24c-359f3d5b9571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
